{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from vargpt_llava.modeling_vargpt_llava import VARGPTLlavaForConditionalGeneration\n",
    "from vargpt_llava.prepare_vargpt_llava import prepare_vargpt_llava \n",
    "from vargpt_llava.processing_vargpt_llava import VARGPTLlavaProcessor\n",
    "from patching_utils.patching import patching\n",
    "model_id = \"VARGPT-family/VARGPT_LLaVA-v1\"\n",
    "prepare_vargpt_llava(model_id)\n",
    "\n",
    "model = VARGPTLlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float32, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(0)\n",
    "\n",
    "patching(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "processor = VARGPTLlavaProcessor.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Please explain the meme in detail.\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "image_file = \"../assets/llava_bench_demo.png\"\n",
    "print(prompt)\n",
    "\n",
    "raw_image = Image.open(image_file)\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.bfloat16)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=512, \n",
    "    do_sample=False)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# some instruction examples:\n",
    "# Please design a drawing of a butterfly on a flower.\n",
    "# Please create a painting of a black weasel is standing in the grass.\n",
    "# Can you generate a rendered photo of a rabbit sitting in the grass.\n",
    "# I need a designed photo of a lighthouse is seen in the distance.\n",
    "# Please create a rendered drawing of an old photo of an aircraft carrier in the water.\n",
    "# Please produce a designed photo of a squirrel is standing in the snow.\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Please design a drawing of a butterfly on a flower\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "\n",
    "inputs = processor(text=prompt, return_tensors='pt').to(0, torch.float32)\n",
    "model._IMAGE_GEN_PATH = \"output.png\"\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=1000, \n",
    "    do_sample=False)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "7b71818b-d211-4c95-99a3-35f3f4787c95",
  "filePath": "/mnt/bn/yufan-lf/pretrained_models/VARGPT_LLaVA-7B-stage3/VARGPT_code/inference.ipynb",
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
